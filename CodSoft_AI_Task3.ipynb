{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2nGy6zftc/9TSbr45eiTp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChetanKrishnaPeela/CodSoft/blob/main/CodSoft_AI_Task3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 0\n",
        "        self.add_word('<pad>')\n",
        "        self.add_word('<start>')\n",
        "        self.add_word('<end>')\n",
        "        self.add_word('<unk>')\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            self.idx += 1\n",
        "\n",
        "    def __call__(self, word):\n",
        "        return self.word2idx.get(word, self.word2idx['<unk>'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)\n",
        "\n",
        "class ImageCaptioningModel(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "        super(ImageCaptioningModel, self).__init__()\n",
        "        self.cnn = models.resnet50(pretrained=True)\n",
        "        self.cnn.fc = nn.Linear(self.cnn.fc.in_features, embed_size)\n",
        "        for param in self.cnn.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(\n",
        "            nn.TransformerDecoderLayer(d_model=embed_size, nhead=8), num_layers\n",
        "        )\n",
        "        self.fc = nn.Linear(embed_size, vocab_size)\n",
        "        self.embed_size = embed_size\n",
        "\n",
        "    def forward(self, images, captions, lengths):\n",
        "        features = self.cnn(images)\n",
        "        features = features.unsqueeze(1)\n",
        "\n",
        "        embeddings = self.embedding(captions)\n",
        "\n",
        "        outputs = self.transformer_decoder(embeddings.transpose(0, 1), features.transpose(0, 1))\n",
        "        outputs = self.fc(outputs.transpose(0, 1))\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def caption_image(self, image, vocab, max_length=20):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            transform = transforms.Compose([\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "            image = transform(image).unsqueeze(0)\n",
        "\n",
        "            features = self.cnn(image).unsqueeze(1)\n",
        "\n",
        "            caption = []\n",
        "            input_token = torch.tensor([vocab('<start>')]).unsqueeze(0)\n",
        "            for _ in range(max_length):\n",
        "                embeddings = self.embedding(input_token)\n",
        "                output = self.transformer_decoder(embeddings.transpose(0, 1), features.transpose(0, 1))\n",
        "                output = self.fc(output.transpose(0, 1))\n",
        "                predicted = output.argmax(2)[:, -1].item()\n",
        "                caption.append(predicted)\n",
        "                if predicted == vocab('<end>'):\n",
        "                    break\n",
        "                input_token = torch.tensor([predicted]).unsqueeze(0)\n",
        "\n",
        "            return [vocab.idx2word[idx] for idx in caption if idx not in [vocab('<start>'), vocab('<end>')]]\n",
        "\n",
        "class MockDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, vocab):\n",
        "        self.vocab = vocab\n",
        "        self.images = [torch.randn(3, 224, 224) for _ in range(10)]\n",
        "        self.captions = [\n",
        "            ['<start>', 'a', 'dog', 'is', 'running', '<end>'],\n",
        "            ['<start>', 'a', 'cat', 'is', 'sleeping', '<end>']\n",
        "        ] * 5\n",
        "        for caption in self.captions:\n",
        "            for word in caption:\n",
        "                vocab.add_word(word)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        caption = [self.vocab(word) for word in self.captions[idx]]\n",
        "        return image, torch.tensor(caption), len(caption)\n",
        "\n",
        "def train_model(model, dataset, num_epochs=5):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab('<pad>'))\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for image, caption, length in torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True):\n",
        "            image, caption = image.to(device), caption.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(image, caption[:, :-1], length)\n",
        "            loss = criterion(outputs.view(-1, len(dataset.vocab)), caption[:, 1:].contiguous().view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    vocab = Vocabulary()\n",
        "    dataset = MockDataset(vocab)\n",
        "\n",
        "    model = ImageCaptioningModel(embed_size=256, hidden_size=512, vocab_size=len(vocab), num_layers=3)\n",
        "\n",
        "    train_model(model, dataset)\n",
        "\n",
        "    sample_image = Image.fromarray((torch.randn(3, 224, 224).numpy() * 255).astype(np.uint8).transpose(1, 2, 0))\n",
        "    caption = model.caption_image(sample_image, vocab)\n",
        "    print('Generated Caption:', ' '.join(caption))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Pq8BHTjgDJ_",
        "outputId": "47ba1e6c-d060-4acb-813f-fcb59d83d615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 89.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 0.4145\n",
            "Epoch [2/5], Loss: 0.0748\n",
            "Epoch [3/5], Loss: 0.0239\n",
            "Epoch [4/5], Loss: 0.0055\n",
            "Epoch [5/5], Loss: 0.0015\n",
            "Generated Caption: a cat is sleeping\n"
          ]
        }
      ]
    }
  ]
}